{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94fca2",
   "metadata": {},
   "source": [
    "# Task 2: RAG + LLM System Design\n",
    "\n",
    "## 1. System Architecture\n",
    "\n",
    "Here is a proposed architecture for the RAG system, including the key components:\n",
    "\n",
    "### Document Ingestion & Preprocessing\n",
    "*   **Purpose:** To load raw technical documentation PDFs and convert them into a structured format suitable for processing.\n",
    "*   **Components:**\n",
    "    *   **PDF Reader:** Reads text content from PDF files. Libraries like `PyMuPDF` or `pdfminer.six` can be used.\n",
    "    *   **Text Extractor:** Extracts plain text from the PDF content.\n",
    "    *   **Cleaner:** Removes irrelevant characters, headers, footers, and performs basic text cleaning (e.g., handling hyphenation, special characters).\n",
    "\n",
    "### Chunking Strategy\n",
    "*   **Purpose:** To break down large documents into smaller, manageable chunks that retain local context.\n",
    "*   **Components:**\n",
    "    *   **Text Splitter:** Splits the cleaned text based on predefined rules (e.g., fixed token size with overlap, paragraph breaks, section headers). Frameworks like `LangChain` or `LlamaIndex` provide various text splitting strategies.\n",
    "\n",
    "### Embeddings & Indexing (Vector Database)\n",
    "*   **Purpose:** To convert text chunks into numerical vector representations (embeddings) and store them in a searchable index.\n",
    "*   **Components:**\n",
    "    *   **Embedding Model:** An open-source model (e.g., from Hugging Face `sentence-transformers`) that converts text chunks into dense vectors.\n",
    "    *   **Vector Database:** A database optimized for storing and searching vector embeddings (e.g., `FAISS`, `Chroma`, `Weaviate` in local mode).\n",
    "\n",
    "### Retrieval Layer\n",
    "*   **Purpose:** To retrieve the most relevant document chunks based on a user's natural language query.\n",
    "*   **Components:**\n",
    "    *   **Query Embedding:** Converts the user's query into a vector using the same embedding model used for document chunks.\n",
    "    *   **Vector Search:** Performs a similarity search in the vector database to find the top-k most similar document chunk embeddings to the query embedding.\n",
    "    *   **Retriever:** Orchestrates the query embedding and vector search to return the relevant document chunks.\n",
    "\n",
    "### LLM Layer for Answer Generation\n",
    "*   **Purpose:** To generate a natural language answer based on the user's query and the retrieved document chunks.\n",
    "*   **Components:**\n",
    "    *   **Open-source LLM:** A free and open-source language model (e.g., `flan-t5-small`, `opt-125m`, Llama 2 7B, or a Hugging Face hosted model).\n",
    "    *   **Prompt Engineering:** Constructs a prompt for the LLM that includes the user's query, the retrieved document chunks (as context), and instructions for generating a concise and faithful answer.\n",
    "    *   **Answer Generator:** Feeds the prompt to the LLM and processes the generated response.\n",
    "\n",
    "### Guardrails for Safe and Faithful Responses\n",
    "*   **Purpose:** To prevent the LLM from generating irrelevant, hallucinated, or sensitive content and to ensure answers are supported by the retrieved sources.\n",
    "*   **Components:**\n",
    "    *   **Relevance Checker:** Filters retrieved chunks to ensure they are highly relevant to the query before passing them to the LLM.\n",
    "    *   **Citation Enforcer:** Develops a prompt structure or post-processing step to encourage or enforce the LLM to cite the source document chunks for its answer.\n",
    "    *   **Sensitivity Filter:** Identifies and potentially blocks or modifies queries or generated answers that contain sensitive or inappropriate content. This could involve keyword matching or a separate classification model.\n",
    "    *   **Fallback Mechanism:** Provides a predefined message when no relevant documents are found for a query.\n",
    "\n",
    "This architecture provides a modular approach to building the RAG system, allowing for flexibility and potential future improvements to individual components. The next steps will detail the specific strategies and tools for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804d450",
   "metadata": {},
   "source": [
    "## 2. Retrieval Strategy\n",
    "\n",
    "This section details the strategies for effectively retrieving relevant document chunks based on a user query.\n",
    "\n",
    "### Document Chunking Approach\n",
    "\n",
    "*   **Strategy:** A balance between retaining local context and managing chunk size for efficient embedding and retrieval is crucial. A **fixed token size with overlap** is a robust starting point.\n",
    "    *   **Size:** Chunks of around 200-500 tokens are generally effective. Smaller chunks might lose context, while larger chunks can introduce irrelevant information and increase computational cost.\n",
    "    *   **Overlap:** An overlap of 10-20% between consecutive chunks helps maintain context across chunk boundaries and prevents information loss.\n",
    "    *   **Granularity:** While fixed-size chunks are simple, exploring **semantic chunking** (splitting based on meaning or topic) using techniques like LangChain's `SemanticChunker` could improve relevance, especially for complex technical documentation. However, for a minimal prototype, fixed-size chunking is sufficient.\n",
    "\n",
    "### Choice of Embedding Model\n",
    "\n",
    "*   **Requirement:** Use a free and open-source model from Hugging Face `sentence-transformers`.\n",
    "*   **Suggested Model:** `all-MiniLM-L6-v2`.\n",
    "    *   **Reasoning:** This model is a good balance of performance and size. It provides good semantic similarity search capabilities and is relatively fast and efficient for local use, aligning with the \"free and open-source\" and \"minimal prototype\" requirements. Other options like `paraphrase-MiniLM-L3-v2` are even smaller and faster but might offer slightly less performance. `all-mpnet-base-v2` is more powerful but also larger.\n",
    "\n",
    "### Retrieval Method\n",
    "\n",
    "*   **Method:** **Dense vector search** using a vector database is the primary retrieval method.\n",
    "    *   **Process:** The user query is embedded into a vector using the same embedding model as the document chunks. A similarity search (e.g., cosine similarity) is performed in the vector database to find the top-k document chunks with the most similar vectors to the query vector.\n",
    "*   **Consideration:** For a more robust system, **hybrid search** combining dense vector search with a lexical search method like **BM25** could be beneficial. This helps capture both semantic meaning and keyword matching. However, for the minimal prototype, dense vector search is sufficient.\n",
    "\n",
    "### Ensuring Relevance and Faithfulness\n",
    "\n",
    "*   **Re-ranking:** After initial retrieval, a re-ranking step can improve the order of the retrieved chunks. Techniques like using a cross-encoder model (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`) can score the relevance of each retrieved chunk to the query more precisely.\n",
    "*   **Enforcing Citations:** This can be done through prompt engineering. Instructing the LLM to cite the source document chunks when generating the answer is a key guardrail against hallucinations. The prompt should clearly present the retrieved chunks and ask the LLM to refer to them.\n",
    "*   **Returning Source Snippets:** The prototype should return the retrieved document chunks alongside the generated answer. This allows users to verify the information and provides transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b485917",
   "metadata": {},
   "source": [
    "## 4. Scalability Considerations\n",
    "\n",
    "This section addresses how the proposed RAG system design would handle increased scale in terms of documents, users, and deployment environments.\n",
    "\n",
    "### Handling a 10x Increase in Documents\n",
    "\n",
    "*   **Vector Database:** The primary component to consider is the vector database. Using a scalable vector database (e.g., cloud-managed solutions or distributed on-premise options) is crucial. The indexing process will take longer, but once indexed, the search efficiency should be maintained by the database's architecture.\n",
    "*   **Ingestion Pipeline:** The document ingestion and preprocessing pipeline should be designed to handle parallel processing of documents. This can be achieved using distributed processing frameworks.\n",
    "*   **Embedding Model:** While `all-MiniLM-L6-v2` is suitable for the prototype, for a 10x increase, a more efficient or distributed embedding process might be needed. Cloud-based embedding APIs or running the model on more powerful hardware can help.\n",
    "\n",
    "### Handling 100+ Concurrent Users\n",
    "\n",
    "*   **Retrieval Layer:** The retrieval layer needs to be able to handle a high volume of concurrent queries. This primarily depends on the performance of the vector database and the efficiency of the query embedding process.\n",
    "*   **LLM Layer:** The LLM layer can become a bottleneck with many concurrent users, especially if using a single instance of a large model.\n",
    "    *   **Scaling LLM Inference:** Several strategies can be employed:\n",
    "        *   **Model Serving Frameworks:** Use frameworks like TensorFlow Serving, PyTorchServe, or NVIDIA Triton Inference Server for efficient model deployment and scaling.\n",
    "        *   **Load Balancing:** Distribute incoming user queries across multiple instances of the LLM.\n",
    "        *   **Quantization and Distillation:** Explore using smaller, more efficient versions of the LLM through techniques like quantization and distillation to reduce inference time and resource requirements.\n",
    "        *   **Batching:** Process multiple user queries in batches to improve efficiency.\n",
    "*   **Caching:** Implement caching at various levels (e.g., caching popular query embeddings or LLM responses) to reduce the load on the system.\n",
    "\n",
    "### Cloud Deployment Under Cost Constraints (Serverless or GPU-Efficient Scaling)\n",
    "\n",
    "*   **Serverless Functions:** For the ingestion and preprocessing pipeline, serverless functions (e.g., AWS Lambda, Google Cloud Functions) can be cost-effective as you only pay for the compute time used.\n",
    "*   **Managed Vector Database:** Cloud providers offer managed vector database services that handle scaling and infrastructure management, reducing operational overhead. Choose options that fit within cost constraints.\n",
    "*   **GPU-Efficient LLM Scaling:**\n",
    "    *   **Choose Efficient Models:** Select LLMs known for their efficiency and lower hardware requirements (e.g., smaller models, quantized models).\n",
    "    *   **Optimize Inference:** Utilize techniques like model quantization, pruning, and efficient inference libraries (e.g., FasterTransformer, ONNX Runtime) to reduce GPU usage and cost.\n",
    "    *   **Spot Instances or Preemptible VMs:** For non-critical or batch processing tasks (like initial indexing), using spot instances or preemptible VMs can significantly reduce compute costs.\n",
    "    *   **Autoscaling:** Configure autoscaling for LLM inference endpoints to dynamically adjust the number of instances based on demand, optimizing cost.\n",
    "\n",
    "By considering these factors, the RAG system can be designed to scale effectively to handle increasing data and user loads within defined cost constraints in a cloud environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5005fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\abhis\\anaconda3\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\abhis\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\abhis\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\abhis\\anaconda3\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: langchain in c:\\users\\abhis\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain) (0.4.23)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.7 MB 2.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.3/18.7 MB 3.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.8/18.7 MB 3.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.8/18.7 MB 3.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.8/18.7 MB 3.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.8/18.7 MB 3.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.4/18.7 MB 1.5 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.1/18.7 MB 1.8 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.9/18.7 MB 2.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 4.5/18.7 MB 2.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 4.7/18.7 MB 2.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.5/18.7 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 6.8/18.7 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 7.3/18.7 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 7.6/18.7 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 7.6/18.7 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 7.6/18.7 MB 2.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 8.1/18.7 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 8.7/18.7 MB 2.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.0/18.7 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 10.5/18.7 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 10.5/18.7 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 10.7/18.7 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 11.3/18.7 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 11.8/18.7 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.6/18.7 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 13.1/18.7 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 13.9/18.7 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 14.2/18.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.7/18.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.9/18.7 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.7/18.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.3/18.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.0/18.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.6/18.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.1/18.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 2.3 MB/s  0:00:08\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF sentence-transformers faiss-cpu transformers torch langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2a708",
   "metadata": {},
   "source": [
    "### Document Ingestion and Chunking\n",
    "\n",
    "We will now load the documents, extract the text, and split it into chunks. For this prototype, you can place your sample PDF documents in a folder named `docs` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d175eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using document directory: docs\n",
      "Processed document: 172-65231ma.pdf\n",
      "Processed document: 43 86_Cyclone Separator_IandO.pdf\n",
      "Processed document: 654b45bb8292f586313470.pdf\n",
      "Processed document: B_12a.pdf\n",
      "Processed document: Cyclone_Emission_Control.pdf\n",
      "Processed document: Cyclone_Manual.pdf\n",
      "Processed document: D13.pdf\n",
      "Processed document: Operating-and-maintenance-manual-for-a-typical-cylcone-seperator-.pdf\n",
      "Processed document: Parker_DustHog_C_Series_Cyclone_Dust_Collector_User_Guide.pdf\n",
      "Processed document: sect7a.pdf\n",
      "Processed document: SOP Cyclone-22022023.pdf\n",
      "\n",
      "Created 695 chunks from 11 documents.\n",
      "\n",
      "First chunk example:\n",
      "172-65231MA-03 (DC7)   6 October 2021 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Cyclone Separator \n",
      "DC7 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Copyright © 2021 by TLV CO., LTD. \n",
      "All rights reserved\n",
      " \n",
      " \n",
      "172-65231MA-03 (DC7)   6 Oct 2021 \n",
      "1\n",
      "Contents \n",
      " \n",
      "Introduction ....................................................................... 1 \n",
      "Safety Considerations ....................................................... 2 \n",
      "Specifications .................................................................... 4\n",
      "Source: 172-65231ma.pdf_chunk_1\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Define the directory where sample documents are stored\n",
    "docs_dir = \"docs\"\n",
    "if not os.path.exists(docs_dir):\n",
    "    os.makedirs(docs_dir)\n",
    "    print(f\"Created directory: {docs_dir}. Please place your sample PDF documents here.\")\n",
    "else:\n",
    "    print(f\"Using document directory: {docs_dir}\")\n",
    "\n",
    "# List to hold the text content of documents\n",
    "documents = []\n",
    "\n",
    "# Iterate through files in the docs directory and extract text from PDFs\n",
    "for filename in os.listdir(docs_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(docs_dir, filename)\n",
    "        try:\n",
    "            with fitz.open(filepath) as doc:\n",
    "                text = \"\"\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "                documents.append({\"filename\": filename, \"content\": text})\n",
    "            print(f\"Processed document: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "if not documents:\n",
    "    print(\"No PDF documents found in the 'docs' directory. Please add some sample PDFs to proceed with chunking and embedding.\")\n",
    "\n",
    "# Initialize the text splitter\n",
    "# Using RecursiveCharacterTextSplitter as it's a good general-purpose splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Defined in retrieval strategy\n",
    "    chunk_overlap=100 # Defined in retrieval strategy\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    doc_chunks = text_splitter.create_documents([doc[\"content\"]])\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        chunks.append({\n",
    "            \"filename\": doc[\"filename\"],\n",
    "            \"chunk_id\": f\"{doc['filename']}_chunk_{i+1}\",\n",
    "            \"content\": chunk.page_content,\n",
    "            \"source\": f\"{doc['filename']}_chunk_{i+1}\" # Add source for citation\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "# Display the first chunk as an example\n",
    "if chunks:\n",
    "    print(\"\\nFirst chunk example:\")\n",
    "    print(chunks[0]['content'])\n",
    "    print(\"Source:\", chunks[0]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ff6b1",
   "metadata": {},
   "source": [
    "### Embedding and Indexing (Vector Database)\n",
    "\n",
    "Now we will convert the document chunks into vector embeddings using a pre-trained model and index them in a local FAISS database for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f542f454",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c100faa5fef74cc7879809b077d67f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976b39d7233c43e19ae275af29ea6a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2edd7b25d944a3c9e890655230fa249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24758b529693493f984cbf5190bc2498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712f3c587177420a990e769224a95b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5af31310a84fbf85be95a984fa6ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740521147281453ba64242f441e03eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798a94d9020d493aa19fd8f4eb88c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d68c4bf415448b098688b00d9d1da76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc34b302a834a2e836c1e790a132aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33764c47d9974994a8b189fe0997d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n",
      "Created embeddings for 695 chunks.\n",
      "FAISS index created with dimension: 384\n",
      "Added 695 embeddings to the index.\n",
      "\n",
      "Embedding and indexing complete.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding model\n",
    "# Using the suggested model from the retrieval strategy\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# Create embeddings for the chunks\n",
    "chunk_contents = [chunk[\"content\"] for chunk in chunks]\n",
    "chunk_embeddings = embedding_model.encode(chunk_contents)\n",
    "print(f\"Created embeddings for {len(chunk_embeddings)} chunks.\")\n",
    "\n",
    "# Convert embeddings to a numpy array with float32 datatype\n",
    "chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index\n",
    "# Using IndexFlatL2 for a simple L2 distance-based index\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "print(f\"FAISS index created with dimension: {chunk_embeddings.shape[1]}\")\n",
    "\n",
    "# Add the embeddings to the index\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"Added {index.ntotal} embeddings to the index.\")\n",
    "\n",
    "# Store chunks and index for later use\n",
    "# In a real application, you would save the index and chunk metadata to disk\n",
    "# and load them when needed. For this prototype, we keep them in memory.\n",
    "indexed_chunks = chunks\n",
    "faiss_index = index\n",
    "\n",
    "print(\"\\nEmbedding and indexing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4634855",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "Now we will implement the retrieval step. Given a user query, we will embed it and search the FAISS index to find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bfccd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 retrieved chunks for the query: 'What does a sudden draft drop indicate?'\n",
      "\n",
      "--- Chunk 1 (Source: SOP Cyclone-22022023.pdf_chunk_31) ---\n",
      "IMD. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Page 7 of 25 \n",
      " \n",
      "7. \n",
      "Flow Chart of actions in case of Cyclone / Storm \n",
      " \n",
      " \n",
      "Figure 1 – Work Flow Chart \n",
      "Page 8 of 25 \n",
      " \n",
      "8. \n",
      "Action of D.G Comm Centre and/or L.R.I.T in case of Cyclone \n",
      "warning: \n",
      "8.1 \n",
      "Receipt of initial warning from IMD (by email). This warning may be that of a \n",
      "‘depression’ which is likely to develop into a Cyclone on a given date and time. \n",
      "8.2 \n",
      "Inform N.A and N.S (Casualty & Response) by phone. \n",
      "8.3\n",
      "\n",
      "--- Chunk 2 (Source: sect7a.pdf_chunk_19) ---\n",
      "particle which has a 50:50 probability of reporting to either the underflow or overflow. \n",
      " \n",
      "Please note that the d50 cutpoint only describes the cyclone.  It has nothing to do with the size grading of \n",
      "the cyclone’s overflow or underflow streams (80% minus 75 microns etc.).  Those gradings are dependent \n",
      "on the size distribution of the feed to the cyclone, and are commonly described as p80, p50, p20 etc. \n",
      " \n",
      " \n",
      "Intro Cyclones And Separators.Doc \n",
      "Page 4  \n",
      "Copyright:  Weir Minerals 2011\n",
      "\n",
      "--- Chunk 3 (Source: SOP Cyclone-22022023.pdf_chunk_86) ---\n",
      "Page 25 of 25 \n",
      " \n",
      " \n",
      "39. CO-ORDINATION WITH \n",
      "DISTRICT / STATE DMA \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "NOTE-1 : the port shall be expected to start sending this information on daily / more frequent intervals to the Ministry of \n",
      "Shipping from the time of hoisting storm warning signal 5. \n",
      "2. Relevant information may be provided in short, specifying date, time, numbers etc.\n",
      "\n",
      "--- Chunk 4 (Source: D13.pdf_chunk_26) ---\n",
      "small drops between 0.5 and 4.0 mm in size. The small drops fall close to the \n",
      "sprinkler whereas the larger ones fall close to the edge of the wetted circle. \n",
      "Drop size is also controlled by pressure and nozzle size. When the pressure is \n",
      "low, drops tend to be much larger as the water jet does not break up easily. So \n",
      "to avoid more wetting of soil use small diameter nozzles operating at or above \n",
      "the normal recommended operating pressure. \n",
      "2.3 FRAME\n",
      "\n",
      "--- Chunk 5 (Source: SOP Cyclone-22022023.pdf_chunk_79) ---\n",
      "OF IMPACT \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "3. LIKELY STORM SURGE (M) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "4. FORECASTED WND SPEED \n",
      "(KM/HR) TILL NEXT REPORT \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "5. PRESENT PORT STORM SIGNAL \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "6. CRISIS MANAGEMENT \n",
      "PROCEDURE ACTIVATED \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "B. MARINE AREAS  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "7. INITIATION OF SHIFTING OUT \n",
      "OF VESSELS (EXPECTED DT. & \n",
      "TIME IF, NOT STARTED) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "8. COMPLETION OF SHIFTING OUT \n",
      "OF VESSELS (EXPECTED DT. & \n",
      "TIME IF, NOT STARTED \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "9. STATUS OF SECURING OF \n",
      "PORT CMFTS/ LAUNCHES ETC. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "10. STATUS OF SECURING OF PVT\n"
     ]
    }
   ],
   "source": [
    "def retrieve_chunks(query, embedding_model, faiss_index, indexed_chunks, k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant chunks for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        embedding_model: The sentence transformer model for embedding.\n",
    "        faiss_index: The FAISS index containing chunk embeddings.\n",
    "        indexed_chunks (list): A list of dictionaries, where each dictionary\n",
    "                                contains the metadata and content of a chunk.\n",
    "        k (int): The number of top relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top-k most relevant chunk dictionaries.\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "\n",
    "    # Perform similarity search in the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the corresponding chunks\n",
    "    retrieved_chunks = [indexed_chunks[i] for i in indices[0]]\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Example usage:\n",
    "query = \"What does a sudden draft drop indicate?\"\n",
    "retrieved_chunks = retrieve_chunks(query, embedding_model, faiss_index, indexed_chunks)\n",
    "\n",
    "print(f\"Top {len(retrieved_chunks)} retrieved chunks for the query: '{query}'\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk['source']}) ---\")\n",
    "    print(chunk['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6cf52",
   "metadata": {},
   "source": [
    "### Answer Generation with LLM\n",
    "\n",
    "Now we will use a free and open-source LLM to generate an answer based on the retrieved document chunks. This step will also demonstrate how to incorporate citations.\n",
    "\n",
    "**Note:** Running a local LLM might require significant computational resources. For this prototype, we will outline the process and use a placeholder for the actual LLM interaction. You can replace this with code to interact with a locally running model (e.g., using the `transformers` library with a downloaded model) or a free/open-source LLM API if available and suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb440621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Answer ---\n",
      "Placeholder Answer: (Replace this with LLM generated text based on the context)\n",
      "\n",
      "Retrieved context used:\n",
      "Source: SOP Cyclone-22022023.pdf_chunk_31\n",
      "IMD. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Page 7 of 25 \n",
      " \n",
      "7. \n",
      "Flow Chart of actions in case of Cyclone / Storm \n",
      " \n",
      " \n",
      "Figure 1 – Work Flow Chart \n",
      "Page 8 of 25 \n",
      " \n",
      "8. \n",
      "Action of D.G Comm Centre and/or L.R.I.T in case of Cyclone \n",
      "warning: \n",
      "8.1 \n",
      "Receipt of initial warning from IMD (by email). This warning may be that of a \n",
      "‘depression’ which is likely to develop into a Cyclone on a given date and time. \n",
      "8.2 \n",
      "Inform N.A and N.S (Casualty & Response) by phone. \n",
      "8.3\n",
      "\n",
      "Source: sect7a.pdf_chunk_19\n",
      "particle which has a 50:50 probability of reporting to either the underflow or overflow. \n",
      " \n",
      "Please note that the d50 cutpoint only describes the cyclone.  It has nothing to do with the size grading of \n",
      "the cyclone’s overflow or underflow streams (80% minus 75 microns etc.).  Those gradings are dependent \n",
      "on the size distribution of the feed to the cyclone, and are commonly described as p80, p50, p20 etc. \n",
      " \n",
      " \n",
      "Intro Cyclones And Separators.Doc \n",
      "Page 4  \n",
      "Copyright:  Weir Minerals 2011\n",
      "\n",
      "Source: SOP Cyclone-22022023.pdf_chunk_86\n",
      "Page 25 of 25 \n",
      " \n",
      " \n",
      "39. CO-ORDINATION WITH \n",
      "DISTRICT / STATE DMA \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "NOTE-1 : the port shall be expected to start sending this information on daily / more frequent intervals to the Ministry of \n",
      "Shipping from the time of hoisting storm warning signal 5. \n",
      "2. Relevant information may be provided in short, specifying date, time, numbers etc.\n",
      "\n",
      "Source: D13.pdf_chunk_26\n",
      "small drops between 0.5 and 4.0 mm in size. The small drops fall close to the \n",
      "sprinkler whereas the larger ones fall close to the edge of the wetted circle. \n",
      "Drop size is also controlled by pressure and nozzle size. When the pressure is \n",
      "low, drops tend to be much larger as the water jet does not break up easily. So \n",
      "to avoid more wetting of soil use small diameter nozzles operating at or above \n",
      "the normal recommended operating pressure. \n",
      "2.3 FRAME\n",
      "\n",
      "Source: SOP Cyclone-22022023.pdf_chunk_79\n",
      "OF IMPACT \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "3. LIKELY STORM SURGE (M) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "4. FORECASTED WND SPEED \n",
      "(KM/HR) TILL NEXT REPORT \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "5. PRESENT PORT STORM SIGNAL \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "6. CRISIS MANAGEMENT \n",
      "PROCEDURE ACTIVATED \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "B. MARINE AREAS  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "7. INITIATION OF SHIFTING OUT \n",
      "OF VESSELS (EXPECTED DT. & \n",
      "TIME IF, NOT STARTED) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "8. COMPLETION OF SHIFTING OUT \n",
      "OF VESSELS (EXPECTED DT. & \n",
      "TIME IF, NOT STARTED \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "9. STATUS OF SECURING OF \n",
      "PORT CMFTS/ LAUNCHES ETC. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "10. STATUS OF SECURING OF PVT\n"
     ]
    }
   ],
   "source": [
    "# This is a placeholder for LLM interaction.\n",
    "# In a real implementation, you would load your chosen free/open LLM here\n",
    "# and use the retrieved_chunks as context for generation.\n",
    "\n",
    "def generate_answer_with_citations(query, retrieved_chunks, llm_model):\n",
    "    \"\"\"\n",
    "    Generates an answer based on the query and retrieved chunks using an LLM,\n",
    "    with an attempt to include citations.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        retrieved_chunks (list): A list of relevant chunk dictionaries.\n",
    "        llm_model: The LLM model to use for generation (placeholder).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer with citations.\n",
    "    \"\"\"\n",
    "    # Construct a prompt for the LLM.\n",
    "    # This prompt instructs the LLM to use the provided context\n",
    "    # and cite the sources.\n",
    "    context = \"\\n\\n\".join([f\"Source: {chunk['source']}\\n{chunk['content']}\" for chunk in retrieved_chunks])\n",
    "    prompt = f\"Based on the following technical documentation, answer the query:\\n\\n{context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    # --- Placeholder for LLM interaction ---\n",
    "    # Replace this with code to call your LLM.\n",
    "    # The LLM should be instructed to generate an answer based *only* on the context\n",
    "    # and to include the source identifier (e.g., filename_chunk_id) when referencing information.\n",
    "    generated_text = f\"Placeholder Answer: (Replace this with LLM generated text based on the context)\\n\\nRetrieved context used:\\n{context}\"\n",
    "    # --- End of Placeholder ---\n",
    "\n",
    "    # In a real scenario, you would process the LLM's output\n",
    "    # to ensure citations are correctly formatted and included.\n",
    "    # For this placeholder, we just return the placeholder text and the context.\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage (using the placeholder):\n",
    "# Replace 'None' with your actual loaded LLM model\n",
    "llm_model = None # Placeholder\n",
    "\n",
    "# Assuming 'retrieved_chunks' is available from the previous step\n",
    "if 'retrieved_chunks' in locals() and retrieved_chunks:\n",
    "    answer = generate_answer_with_citations(query, retrieved_chunks, llm_model)\n",
    "    print(\"\\n--- Generated Answer ---\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"\\nNo chunks retrieved in the previous step to generate an answer from.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2ba0d",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6989d87",
   "metadata": {},
   "source": [
    "# **FUll CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc39344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the complete code for the minimal runnable RAG prototype.\n",
    "# You can copy this code and save it as a Python notebook (e.g., rag_demo.ipynb)\n",
    "# in the 'prototype/' folder.\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "docs_dir = \"docs\"\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "embedding_model_name = 'all-MiniLM-L6-v2'\n",
    "retrieval_k = 5\n",
    "\n",
    "# --- 1. Document Ingestion and Chunking ---\n",
    "\n",
    "print(\"--- Document Ingestion and Chunking ---\")\n",
    "\n",
    "if not os.path.exists(docs_dir):\n",
    "    os.makedirs(docs_dir)\n",
    "    print(f\"Created directory: {docs_dir}. Please place your sample PDF documents here.\")\n",
    "    documents = [] # No documents to process if directory was just created\n",
    "else:\n",
    "    print(f\"Using document directory: {docs_dir}\")\n",
    "    documents = []\n",
    "    for filename in os.listdir(docs_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(docs_dir, filename)\n",
    "            try:\n",
    "                with fitz.open(filepath) as doc:\n",
    "                    text = \"\"\n",
    "                    for page in doc:\n",
    "                        text += page.get_text()\n",
    "                    documents.append({\"filename\": filename, \"content\": text})\n",
    "                print(f\"Processed document: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "if not documents:\n",
    "    print(\"No PDF documents found in the 'docs' directory. Please add some sample PDFs to proceed with chunking and embedding.\")\n",
    "    chunks = [] # No chunks if no documents\n",
    "else:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        doc_chunks = text_splitter.create_documents([doc[\"content\"]])\n",
    "        for i, chunk in enumerate(doc_chunks):\n",
    "            chunks.append({\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"chunk_id\": f\"{doc['filename']}_chunk_{i+1}\",\n",
    "                \"content\": chunk.page_content,\n",
    "                \"source\": f\"{doc['filename']}_chunk_{i+1}\"\n",
    "            })\n",
    "    print(f\"Created {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "    if chunks:\n",
    "        print(\"First chunk example:\")\n",
    "        print(chunks[0]['content'])\n",
    "        print(\"Source:\", chunks[0]['source'])\n",
    "\n",
    "# --- 2. Embedding and Indexing (Vector Database) ---\n",
    "\n",
    "print(\"\\n--- Embedding and Indexing ---\")\n",
    "\n",
    "if chunks:\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    print(f\"Embedding model '{embedding_model_name}' loaded.\")\n",
    "\n",
    "    chunk_contents = [chunk[\"content\"] for chunk in chunks]\n",
    "    chunk_embeddings = embedding_model.encode(chunk_contents)\n",
    "    print(f\"Created embeddings for {len(chunk_embeddings)} chunks.\")\n",
    "\n",
    "    chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "    faiss_index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "    print(f\"FAISS index created with dimension: {chunk_embeddings.shape[1]}\")\n",
    "\n",
    "    faiss_index.add(chunk_embeddings)\n",
    "    print(f\"Added {faiss_index.ntotal} embeddings to the index.\")\n",
    "\n",
    "    indexed_chunks = chunks # Store chunks with index\n",
    "    print(\"Embedding and indexing complete.\")\n",
    "else:\n",
    "    print(\"Skipping embedding and indexing as no chunks were created.\")\n",
    "    faiss_index = None\n",
    "    indexed_chunks = []\n",
    "\n",
    "# --- 3. Retrieval ---\n",
    "\n",
    "print(\"\\n--- Retrieval ---\")\n",
    "\n",
    "def retrieve_chunks(query, embedding_model, faiss_index, indexed_chunks, k=5):\n",
    "    if faiss_index is None or not indexed_chunks:\n",
    "        print(\"Index is not available or no chunks are indexed.\")\n",
    "        return []\n",
    "\n",
    "    query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    retrieved_chunks = [indexed_chunks[i] for i in indices[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# --- 4. Answer Generation with LLM (Placeholder) ---\n",
    "\n",
    "print(\"\\n--- Answer Generation (Placeholder) ---\")\n",
    "\n",
    "def generate_answer_with_citations(query, retrieved_chunks, llm_model):\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant information found in the documents.\"\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Source: {chunk['source']}\\n{chunk['content']}\" for chunk in retrieved_chunks])\n",
    "    prompt = f\"Based on the following technical documentation, answer the query:\\n\\n{context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    # --- Placeholder for LLM interaction ---\n",
    "    # Replace this with code to call your LLM.\n",
    "    # Example using a hypothetical local LLM:\n",
    "    # from transformers import pipeline\n",
    "    # llm = pipeline(\"text-generation\", model=\"your-local-llm-name\")\n",
    "    # generated_text = llm(prompt, max_length=500)[0]['generated_text']\n",
    "    # --- End of Placeholder ---\n",
    "\n",
    "    # For the prototype, we'll just show the context that would be used\n",
    "    generated_text = f\"Placeholder Answer: (Replace this with LLM generated text based on the context)\\n\\nRetrieved context used:\\n{context}\"\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "if faiss_index is not None and indexed_chunks:\n",
    "    query = \"What does a sudden draft drop indicate?\" # replace it with input() for dynamic queries\n",
    "    print(f\"Query: '{query}'\")\n",
    "    retrieved_chunks = retrieve_chunks(query, embedding_model, faiss_index, indexed_chunks, k=retrieval_k)\n",
    "\n",
    "    print(f\"\\nTop {len(retrieved_chunks)} retrieved chunks:\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "         print(f\"--- Chunk {i+1} (Source: {chunk['source']}) ---\")\n",
    "         print(chunk['content'])\n",
    "\n",
    "    # Example LLM interaction (using placeholder)\n",
    "    llm_model = None # Replace with your loaded LLM\n",
    "    answer = generate_answer_with_citations(query, retrieved_chunks, llm_model)\n",
    "    print(\"\\n--- Generated Answer ---\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"\\nSkipping retrieval and answer generation as no documents were processed.\")\n",
    "\n",
    "# --- 5. Evaluation Outline ---\n",
    "print(\"\\n--- Evaluation Outline ---\")\n",
    "print(\"Refer to the 'Evaluation (Conceptual Outline)' section in the notebook/notes.md for details on how to evaluate the prototype.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939cfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
